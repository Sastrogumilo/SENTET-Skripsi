{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Test Model","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOtEytNLLJTON3tqqGt6vPs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"NWodNb6RqhNs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":462},"executionInfo":{"status":"ok","timestamp":1596682934185,"user_tz":-420,"elapsed":102876,"user":{"displayName":"Nala Adina","photoUrl":"","userId":"05330101943390940711"}},"outputId":"7bfaabdd-9d05-4c89-daa7-ff57b73454c7","tags":[]},"source":["import re\n","from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n","\n","import nltk\n","\n","nltk.download('punkt')\n","\n","from pathlib import Path\n","import sys\n","import itertools\n","\n","from pylab import savefig\n","\n","#!pip install -q tensorflow-model-analysis\n","from gensim.models import KeyedVectors\n","#import tensorflow as tf\n","#import tensorflow_model_analysis as tfma\n","\n","#===============================================================================\n","#Tensorflow and Keras \n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n"," \n","from keras.preprocessing.text import one_hot, Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential\n","from keras.layers import Activation, Dropout, Dense, Flatten, LSTM, \\\n","  SpatialDropout1D, Bidirectional, GlobalMaxPooling1D, Conv2D, MaxPool2D,\\\n","  MaxPooling1D, Conv1D, Input, Reshape,Concatenate, Embedding, BatchNormalization,\\\n","  GRU\n","\n","from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping,\\\n","    ReduceLROnPlateau\n","\n","from keras.models import Model\n","\n","from keras.metrics import MeanIoU\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.initializers import Constant\n","\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","\n","#===============================================================================\n","import sklearn.metrics as skm\n","\n","from sklearn.model_selection import KFold, StratifiedKFold\n","\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import f1_score,precision_score,recall_score,jaccard_score, hamming_loss, accuracy_score \n","from sklearn.metrics import multilabel_confusion_matrix\n","from sklearn.utils import class_weight\n","from sklearn.model_selection import train_test_split\n","from skmultilearn.model_selection import IterativeStratification\n","from skmultilearn.problem_transform import LabelPowerset\n","\n","import os\n","import datetime\n","\n","\n","import pandas as pd\n","\n","import numpy as np\n","from numpy import array\n","\n","import re\n","\n","import matplotlib.pyplot as plt\n","\n","from IPython.display import Markdown, display\n","def printmd(string):\n","    display(Markdown(string))\n","#printmd('**bold**')\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import string\n","import nltk\n","\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Belldandy\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nUsing TensorFlow backend.\n2.0.0\n"}]},{"cell_type":"code","metadata":{"id":"zHUtwteTrJ-7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1596682934186,"user_tz":-420,"elapsed":102869,"user":{"displayName":"Nala Adina","photoUrl":"","userId":"05330101943390940711"}},"outputId":"3ab78200-bc3a-4668-8038-b6251d4d1c19","tags":[]},"source":["import re\n","id_stopword_dict = pd.read_csv('stopword.csv', header=None)\n","id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})\n","\n","factory = StemmerFactory()\n","stemmer = factory.create_stemmer()\n","\n","alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n","alay_dict = alay_dict.rename(columns={0: 'original', \n","                                      1: 'replacement'})\n","\n","stopword = 'stopword.txt'\n","def lowercase(text):\n","    return text.lower()\n","\n","def remove_unnecessary_char(text):\n","    #\\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84'\n","    text = re.sub('\\n',' ',text) # Remove every '\\n'\n","    text = re.sub('rt',' ',text) # Remove every retweet symbol\n","    text = re.sub('xf0',' ',text) # Remove every retweet symbol\n","    text = re.sub('x9f',' ',text) # Remove every retweet symbol\n","    text = re.sub('x98',' ',text) # Remove every retweet symbol\n","    text = re.sub('x82',' ',text) # Remove every retweet symbol\n","    text = re.sub('x84',' ',text) # Remove every retweet symbol\n","    text = re.sub('xe2',' ',text) # Remove every retweet symbol\n","    text = re.sub('x80',' ',text) # Remove every retweet symbol\n","    text = re.sub('amp',' ',text) # Remove every retweet symbolxa6\n","    text = re.sub('xa6',' ',text) # Remove every retweet symbol xa4\n","    text = re.sub('xa4',' ',text) # Remove every retweet symbol\n","\n","    text = re.sub('user',' ',text) # Remove every username\n","    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n","    text = re.sub('  +', ' ', text) # Remove extra spaces\n","    return text\n","    \n","def remove_nonaplhanumeric(text):\n","    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n","    return text\n","\n","alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n","def normalize_alay(text):\n","    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n","\n","def remove_stopword(text):\n","    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n","    text = re.sub('  +', ' ', text) # Remove extra spaces\n","    text = text.strip()\n","    return text\n","    \n","    return \"\".join([\"\"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in filtered_sentence]).strip()\n","\n","def stemming(text):\n","    return stemmer.stem(text)\n","\n","print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa!!\"))\n","print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n","print(\"stemming: \", stemming(\"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"))\n","print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe\"))\n","print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))\n","print(\"remove_stopword: \", remove_stopword(\"ada hehe adalah huhu yang hehe\"))\n","\n","def preprocess(text):\n","    text = lowercase(text) # 1\n","    text = remove_nonaplhanumeric(text) # 2\n","    text = remove_unnecessary_char(text) # 2\n","    text = normalize_alay(text) # 3\n","    text = stemming(text) # 4\n","    text = remove_stopword(text) # 5\n","    return text"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":"remove_nonaplhanumeric:  Halooo duniaa \nlowercase:  halooo, duniaa!\nstemming:  ekonomi indonesia sedang dalam tumbuh yang bangga\nremove_unnecessary_char:  Hehe RT USER USER apa kabs hehe\nnormalize_alay:  amin adik habis\nremove_stopword:  hehe huhu hehe\n"}]},{"cell_type":"code","metadata":{"id":"C67sKyB7q8v9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596682935350,"user_tz":-420,"elapsed":1161,"user":{"displayName":"Nala Adina","photoUrl":"","userId":"05330101943390940711"}}},"source":["path_main = \"./Model/\"\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"tNDgvwUfqnbw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596682951591,"user_tz":-420,"elapsed":16231,"user":{"displayName":"Nala Adina","photoUrl":"","userId":"05330101943390940711"}},"outputId":"baa8a1d3-f8e2-45d4-8e71-e46aac326740","tags":[]},"source":["from keras.models import load_model\n","import pickle\n","maxlen = 200\n","tokenizer = Tokenizer(5000)\n","\n","with open('tokenizer.pickle', 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","\n","#categories = list(data.columns.values)\n","#categories = categories[1:]\n","#print(categories)\n","\n","\n","main = load_model(path_main+'Main_BiLSTM_Att.h5')\n","print(\"Model Main Loaded !\")\n","\n","target = load_model(path_main+'Target_BiLSTM_Att.h5')\n","print(\"Model Target Loaded !\")\n","\n","tipe = load_model(path_main+'Type_BiLSTM_Att.h5')\n","print(\"Model Tipe Loaded !\")\n","\n","level = load_model(path_main+'Level_BiLSTM_Att.h5')\n","print(\"Model Level Loaded !\")\n","\n","#%cd \"/content/drive/\"\n","\n","def prediction_to_label_main(prediction):\n","    categories = [\"HS\", \"Abusive\"]\n","    tag_prob = [(categories[i], prob) for i, prob in enumerate(prediction.tolist())]\n","    return dict(sorted(tag_prob, key=lambda kv: kv[1], reverse=False))\n","\n","def prediction_to_label_target(prediction):\n","    categories = [\"HS_Individu\", \"HS_Group\"]\n","    tag_prob = [(categories[i], prob) for i, prob in enumerate(prediction.tolist())]\n","    return dict(sorted(tag_prob, key=lambda kv: kv[1], reverse=False))\n","\n","def prediction_to_label_type(prediction):\n","    categories = ['HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other']\n","    tag_prob = [(categories[i], prob) for i, prob in enumerate(prediction.tolist())]\n","    return dict(sorted(tag_prob, key=lambda kv: kv[1], reverse=False))\n","\n","def prediction_to_label_level(prediction):\n","    categories = ['HS_Weak', 'HS_Moderate', \"HS_Strong\"]\n","    tag_prob = [(categories[i], prob) for i, prob in enumerate(prediction.tolist())]\n","    return dict(sorted(tag_prob, key=lambda kv: kv[1], reverse=False))\n","\n","def get_features(text_series):\n","    \"\"\"\n","    transforms text data to feature_vectors that can be used in the ml model.\n","    tokenizer must be available.\n","    \"\"\"\n","    sequences = tokenizer.texts_to_sequences(text_series)\n","    return pad_sequences(sequences, maxlen=maxlen, padding='pre')\n","    \n","print(\"======= Notif: Loaded Model Done !========\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":"Model Main Loaded !\nModel Target Loaded !\nModel Tipe Loaded !\nModel Level Loaded !\n"}]},{"cell_type":"code","metadata":{"id":"TsxUdmOFsbPb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596683377586,"user_tz":-420,"elapsed":939,"user":{"displayName":"Nala Adina","photoUrl":"","userId":"05330101943390940711"}}},"source":["def test(f):\n","  sentiment = []\n","  f = preprocess(text)\n","  f =  get_features([f])\n","  def sa():    \n","    pred_main = prediction_to_label_main(main.predict(f)[0])\n","    for label, nilai in pred_main.items():\n","      if nilai >= 0.5:\n","        sentiment.append(label)\n","\n","        pred_target = prediction_to_label_target(target.predict(f)[0])\n","        for label_target, nilai_target in pred_target.items():\n","          if nilai_target >= 0.5:\n","            sentiment.append(label_target)\n","          else:\n","            None\n","\n","        pred_type = prediction_to_label_type(tipe.predict(f)[0])\n","        for label_tipe, nilai_tipe in pred_type.items():\n","          if nilai_tipe >= 0.5:\n","            sentiment.append(label_tipe)\n","          else:\n","            None\n","\n","        pred_level = prediction_to_label_level(level.predict(f)[0])\n","        for label_level, nilai_level in pred_level.items():\n","          if nilai_level >= 0.5:\n","            sentiment.append(label_level)\n","          else:\n","            None      \n","\n","      else:\n","        sentiment.append(\"Normal\")\n","    \n","    return sentiment\n","\n","\n","  def double_check(temp1):\n","    for x in temp1:\n","      temp2 = temp1[:] # create a real copy of temp1\n","      temp2.remove(x)  # remove x so we won't consider it as dup of itself\n","      for y in temp2:\n","          if set(x) == set(y):\n","              temp1.remove(x)\n","    \n","    return hasil\n","\n","  def triple_check(temp1):\n","    if len(temp1) <= 1:\n","      new_list = temp1\n","    else:\n","      new_list = list(filter(lambda x : x != \"Normal\", temp1))\n","    return new_list\n","\n","  hasil = sa()\n","  hasil = double_check(hasil)\n","  hasil = triple_check(hasil)\n","  return hasil"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"OUz5DY8HM7Sh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596683460113,"user_tz":-420,"elapsed":1454,"user":{"displayName":"Nala Adina","photoUrl":"","userId":"05330101943390940711"}},"outputId":"1704d998-409d-4891-bbac-f0658ac75817","tags":[]},"source":["text = 'USER USER USER Makanya cebong ber IQ200 sekolam, karena gak bisa bedain fakta dengan provokasi, gak bisa bedain fakta dgn fitnah'\n","\n","f = preprocess(text)\n","f =  get_features([f])\n","hasil = test(text)\n","print(hasil)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":"['HS', 'HS_Group', 'HS_Other', 'HS_Strong']\n"}]},{"cell_type":"code","metadata":{"id":"cK3TNLCdtsYY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"executionInfo":{"status":"ok","timestamp":1596683460721,"user_tz":-420,"elapsed":1351,"user":{"displayName":"Nala Adina","photoUrl":"","userId":"05330101943390940711"}},"outputId":"414fbf1f-16aa-4ebc-bb6b-fd51ee23a772","tags":[]},"source":["\n","pred_main = prediction_to_label_main(main.predict(f)[0])\n","print(pred_main)\n","\n","pred_target = prediction_to_label_target(target.predict(f)[0])\n","print(pred_target)\n","\n","pred_type = prediction_to_label_type(tipe.predict(f)[0])\n","print(pred_type)\n","\n","pred_level = prediction_to_label_level(level.predict(f)[0])\n","print(pred_level)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"{'Abusive': 0.03409159928560257, 'HS': 0.855967104434967}\n{'HS_Group': 0.17336362600326538, 'HS_Individu': 0.7117328643798828}\n{'HS_Physical': 5.453824996948242e-06, 'HS_Gender': 1.0222196578979492e-05, 'HS_Race': 0.00026670098304748535, 'HS_Religion': 0.0036801695823669434, 'HS_Other': 0.9714818596839905}\n{'HS_Strong': 0.010158302262425423, 'HS_Weak': 0.20429293811321259, 'HS_Moderate': 0.517462432384491}\n"}]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}]}