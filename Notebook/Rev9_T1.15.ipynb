{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Rev8.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OBxqQHpnN32n",
        "lbrpCD-LGWVK",
        "We8rjQIYaLD-",
        "6zHqH1zueUbe",
        "cBGnvDAYR6MP",
        "6NWLX9okmoXU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32XbDfhcGkJr",
        "colab_type": "text"
      },
      "source": [
        "#Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U1ZLtb8FoM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f8bf274f-5c68-483e-8de6-bc42b4eaa092",
        "tags": []
      },
      "source": [
        "\n",
        "import re\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "import itertools\n",
        "\n",
        "from pylab import savefig\n",
        "\n",
        "#!pip install -q tensorflow-model-analysis\n",
        "from gensim.models import KeyedVectors\n",
        "#import tensorflow as tf\n",
        "#import tensorflow_model_analysis as tfma\n",
        "\n",
        "#===============================================================================\n",
        "#Tensorflow and Keras \n",
        "\n",
        "#print('TFMA version: {}'.format(tfma.version.VERSION_STRING))\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "#import tensorflow_addons as tfa\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
        "\n",
        "from keras.preprocessing.text import one_hot, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dropout, Dense, Flatten, LSTM, \\\n",
        "  SpatialDropout1D, Bidirectional, GlobalMaxPooling1D, Conv2D, MaxPool2D,\\\n",
        "  MaxPooling1D, Conv1D, Input, Reshape,Concatenate, Embedding, BatchNormalization,\\\n",
        "  GRU, GlobalAveragePooling1D\n",
        "\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping,\\\n",
        "    ReduceLROnPlateau\n",
        "\n",
        "from keras.layers import InputSpec, Layer\n",
        "from keras import Model\n",
        "\n",
        "from keras.metrics import MeanIoU\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.initializers import Constant\n",
        "\n",
        "from tensorflow.python.client import device_lib \n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "\n",
        "#===============================================================================\n",
        "import sklearn.metrics as skm\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score,precision_score,recall_score,jaccard_score, hamming_loss, accuracy_score \n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skmultilearn.model_selection import IterativeStratification\n",
        "from skmultilearn.problem_transform import LabelPowerset\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "#import extra_keras_metrics\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "import re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    display(Markdown(string))\n",
        "#printmd('**bold**')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 12830073960840793827\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 3141979340\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 17955292172858815980\nphysical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:07:00.0, compute capability: 6.1\"\n]\n2.0.0\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Belldandy\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUaC_NZlFA2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#data = pd.read_csv('data2.csv', encoding='latin-1')\n",
        "data = pd.read_csv('dataset_asli.csv', encoding='latin-1')\n",
        "\n",
        "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original', \n",
        "                                      1: 'replacement'})\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G2EmXkEFPNW",
        "colab_type": "text"
      },
      "source": [
        "#Preproses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBxqQHpnN32n",
        "colab_type": "text"
      },
      "source": [
        "##My Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c31z4usON8LI",
        "colab_type": "text"
      },
      "source": [
        "##Jurnal Based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf_j6uUtFOeu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9cbfdd65-91dd-47e5-f545-3f11ead7f918",
        "tags": []
      },
      "source": [
        "import re\n",
        "id_stopword_dict = pd.read_csv('stopword.csv', header=None)\n",
        "id_stopword_dict = id_stopword_dict.rename(columns={0: 'stopword'})\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "stopword = 'stopword.txt'\n",
        "def lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_unnecessary_char(text):\n",
        "    #\\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84\\xf0\\x9f\\x98\\x84'\n",
        "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
        "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('xf0',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('x9f',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('x98',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('x82',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('x84',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('xe2',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('x80',' ',text) # Remove every retweet symbol\n",
        "    text = re.sub('amp',' ',text) # Remove every retweet symbolxa6\n",
        "    text = re.sub('xa6',' ',text) # Remove every retweet symbol xa4\n",
        "    text = re.sub('xa4',' ',text) # Remove every retweet symbol\n",
        "\n",
        "    text = re.sub('user',' ',text) # Remove every username\n",
        "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
        "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "    return text\n",
        "    \n",
        "def remove_nonaplhanumeric(text):\n",
        "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
        "    return text\n",
        "\n",
        "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
        "def normalize_alay(text):\n",
        "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
        "\n",
        "def remove_stopword(text):\n",
        "    text = ' '.join(['' if word in id_stopword_dict.stopword.values else word for word in text.split(' ')])\n",
        "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
        "    text = text.strip()\n",
        "    return text\n",
        "    \n",
        "    return \"\".join([\"\"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in filtered_sentence]).strip()\n",
        "\n",
        "def stemming(text):\n",
        "    return stemmer.stem(text)\n",
        "\n",
        "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa!!\"))\n",
        "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
        "print(\"stemming: \", stemming(\"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"))\n",
        "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe\"))\n",
        "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))\n",
        "print(\"remove_stopword: \", remove_stopword(\"ada hehe adalah huhu yang hehe\"))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "remove_nonaplhanumeric:  Halooo duniaa \nlowercase:  halooo, duniaa!\nstemming:  ekonomi indonesia sedang dalam tumbuh yang bangga\nremove_unnecessary_char:  Hehe RT USER USER apa kabs hehe\nnormalize_alay:  amin adik habis\nremove_stopword:  hehe huhu hehe\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPx-TgchFTAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(text):\n",
        "    text = lowercase(text) # 1\n",
        "    text = remove_nonaplhanumeric(text) # 2\n",
        "    text = remove_unnecessary_char(text) # 2\n",
        "    text = normalize_alay(text) # 3\n",
        "    text = stemming(text) # 4\n",
        "    text = remove_stopword(text) # 5\n",
        "    return text"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqm0z5nMZtlr",
        "colab_type": "text"
      },
      "source": [
        "##Functional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2Fep-RqZseV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f_score(X, Y, model, iter, name):\n",
        "\n",
        "  predictions=model.predict([X])\n",
        "  thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "  y_true = Y\n",
        "  thres = 1\n",
        "\n",
        "  #Path(pic+versi+\"/\"+name).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  for val in thresholds:\n",
        "    pred = predictions.copy()\n",
        "\n",
        "    pred[pred>=val]=1\n",
        "    pred[pred<val]=0\n",
        "\n",
        "    #y_true = Y\n",
        "    y_pred = pred\n",
        "\n",
        "    cm = skm.multilabel_confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    true = np.random.randint(0, 20, size=100)\n",
        "    pred = np.random.randint(0, 20, size=100)\n",
        "    labels = np.arange(12)\n",
        "\n",
        "    target_names = categories\n",
        "\n",
        "    clf_report = classification_report(y_true,\n",
        "                                    y_pred,\n",
        "                                    labels=labels,\n",
        "                                    target_names=target_names,\n",
        "                                    output_dict=True)\n",
        "    \n",
        "    plt.figure(figsize = (10,5))\n",
        "    # .iloc[:-1, :] to exclude support\n",
        "    fig = sns.heatmap(pd.DataFrame(clf_report).iloc[:-1, :].T, annot=True)\n",
        "    fig.plot()\n",
        "    #sns.heatmap(pd.DataFrame(clf_report).T, annot=True)\n",
        "\n",
        "    fig.figure.savefig(pic+versi+\"/\"+name+\"/\"+str(iter)+\"_\"+name+\"_thresh_\"+str(thres)+\"_f_score\", dpi=640)\n",
        "    \n",
        "    print(\"Gambar untuk Threshold {} telah di export\".format(thres))\n",
        "    thres += 1\n",
        "    \n",
        "\n",
        "  #fig.close('all')\n",
        "  #sns.savefig(pic+iter+name)\n",
        "  #figure.savefig(pic+iter+name)\n",
        "\n",
        "  #print(\"Menyimpan Gambar \" +pic+versi+\"/\"+str(iter)+name+\"_f_score\")\n",
        "  #print(cm)\n",
        "  #print(skm.classification_report(y_true,y_pred))\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title, ax):\n",
        "\n",
        "    ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        ax.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > cm.max() / 2. else \"black\")\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    ax.set_xticks(tick_marks), ax.xaxis.set_ticklabels(classes)\n",
        "    ax.set_yticks(tick_marks), ax.yaxis.set_ticklabels(classes)\n",
        "    ax.set_xlabel('Predicted')\n",
        "    ax.set_ylabel('Truth')\n",
        "    ax.set_title(title)\n",
        "    ax.grid(False)\n",
        "    \n",
        "def plot_multiclass_confusion_matrix(y_true, y_pred, iter, label_to_class, name, save_plot=False):\n",
        "    fig, axes = plt.subplots(int(np.ceil(len(label_to_class) / 4)), 4, figsize=(10, 10))\n",
        "    axes = axes.flatten()\n",
        "    for i, conf_matrix in enumerate(multilabel_confusion_matrix(y_true, y_pred)):\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "        f1 = 2 * tp / (2 * tp + fp + fn + sys.float_info.epsilon)\n",
        "        recall = tp / (tp + fn + sys.float_info.epsilon)\n",
        "        precision = tp / (tp + fp + sys.float_info.epsilon)\n",
        "        plot_confusion_matrix(\n",
        "            np.array([[tp, fn], [fp, tn]]),\n",
        "            classes=['+', '-'],\n",
        "            title=f'Label: {label_to_class[i]}\\nf1={f1:.5f}\\nrecall={recall:.5f}\\nprecision={precision:.5f}',\n",
        "            ax=axes[i]\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "    if save_plot:\n",
        "        plt.savefig(pic+versi+\"/\"+str(iter)+\"_\"+name+\"_CM\", dpi=640)\n",
        "        plt.close('all')\n",
        "    print(\"Graph {} telah diexport\".format(name))\n",
        "\n",
        "\n",
        "def jaccard_distance(y_true, y_pred, smooth=0):\n",
        "    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n",
        "    sum_ = K.sum(K.abs(y_true) + K.abs(y_pred), axis=-1)\n",
        "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
        "    return jac\n",
        "\n",
        "def hamming_test_2(y_true, y_pred):\n",
        "  return (y_true == y_pred).mean()\n",
        "\n",
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    '''\n",
        "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
        "    https://stackoverflow.com/q/32239577/395857\n",
        "    '''\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        #print('\\nset_true: {0}'.format(set_true))\n",
        "        #print('set_pred: {0}'.format(set_pred))\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        #print('tmp_a: {0}'.format(tmp_a))\n",
        "        acc_list.append(tmp_a)\n",
        "\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def presisi(y_true, y_pred,):\n",
        "    '''\n",
        "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
        "    https://stackoverflow.com/q/32239577/395857\n",
        "    '''\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        #print('\\nset_true: {0}'.format(set_true))\n",
        "        #print('set_pred: {0}'.format(set_pred))\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    len(y_true)\n",
        "        #print('tmp_a: {0}'.format(tmp_a))\n",
        "        acc_list.append(tmp_a)\n",
        "\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def rekal(y_true, y_pred,):\n",
        "    '''\n",
        "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
        "    https://stackoverflow.com/q/32239577/395857\n",
        "    '''\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        #print('\\nset_true: {0}'.format(set_true))\n",
        "        #print('set_pred: {0}'.format(set_pred))\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                     len(y_pred)\n",
        "        #print('tmp_a: {0}'.format(tmp_a))\n",
        "        acc_list.append(tmp_a)\n",
        "\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def f1_skor(y_true, y_pred,):\n",
        "\n",
        "    #f1 = (2*precision*recall)/(precision+recall+K.epsilon())\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        #print('\\nset_true: {0}'.format(set_true))\n",
        "        #print('set_pred: {0}'.format(set_pred))\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a =  (len (2*(set_true.intersection(set_pred))))/\\\n",
        "                    len(y_true) + len(y_pred)\n",
        "        #print('tmp_a: {0}'.format(tmp_a))\n",
        "        acc_list.append(tmp_a)\n",
        "\n",
        "    return np.mean(acc_list)\n",
        "\n",
        "def metric_sklearn(X, Y, model, name, iter):\n",
        "\n",
        "    #Path(fitur+versi+\"/\"+name).mkdir(parents=True, exist_ok=True) \n",
        "    predictions=model.predict([X])\n",
        "    thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "    y_true = Y\n",
        "    thres = 1\n",
        "\n",
        "    StoreData = pd.DataFrame(columns=['Thresholds','Precision Micro','Precision Macro', 'Recall Micro','Recall Macro', 'F1-measure Micro','F1-measure Macro','Jaccard', 'Hamming Loss', 'Hamming Score','Exact Accuracy'], dtype=float).fillna('0')\n",
        "    \n",
        "    for val in thresholds:\n",
        "        print(\"For threshold: \", val)\n",
        "        pred=predictions.copy()\n",
        "      \n",
        "        pred[pred>=val]=1\n",
        "        pred[pred<val]=0\n",
        "      \n",
        "        precision = precision_score(y_true, pred, average='micro')\n",
        "        recall = recall_score(y_true, pred, average='micro')\n",
        "        f1 = f1_score(y_true, pred, average='micro')\n",
        "        precision_m = precision_score(y_true, pred, average='macro')\n",
        "        recall_m = recall_score(y_true, pred, average='macro')\n",
        "        f1_m = f1_score(y_true, pred, average='macro')\n",
        "        jaccard = jaccard_score(y_true, pred, average='micro')\n",
        "        haming_losses = hamming_loss(y_true, pred)\n",
        "        acc = accuracy_score(y_true, pred, normalize=True)\n",
        "        hamming_scores = hamming_score(y_true, pred)\n",
        "        hamming_score_2 = hamming_test_2(y_true, pred)\n",
        "        #acc_scores = acc_score(y_true, pred)\n",
        "        #f1_baru = f1_skor(y_true, pred,)\n",
        "        #presisi_baru = presisi(y_true, pred)\n",
        "        #recall_baru = rekal(y_true, pred)\n",
        "\n",
        "        #StoreData = pd.DataFrame(columns=['Thresholds','Precision', 'Recall', 'F1-measure', 'Jaccard', 'Hamming Loss', 'Hamming Score','Exact Accuracy'], dtype=float).fillna('0')\n",
        "        StoreData = StoreData.append(pd.DataFrame({\n",
        "                        'Thresholds':[val],\n",
        "                        'Precision Micro':[precision],\n",
        "                        'Precision Macro':[precision_m],\n",
        "                        'Recall Micro':[recall],\n",
        "                        'Recall Macro':[recall_m],\n",
        "                        'F1-measure Micro':[f1],         \n",
        "                        'F1-measure Macro':[f1_m],\n",
        "                        'Jaccard':[jaccard],\n",
        "                        'Hamming Loss':[haming_losses],\n",
        "                        'Hamming Score':[hamming_scores],\n",
        "                        'Exact Accuracy':[acc],\n",
        "                        }))\n",
        "        thres += 1\n",
        "        print(\"Micro-average quality numbers\")\n",
        "        print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}, Jaccard: {:.4f}, Hamming Loss: {:.4f}, Hamming Score: {:.4f}, Hamming Score 2: {:.4f}, Accuracy: {:.4f},\"\\\n",
        "              .format(precision, recall, f1, jaccard, haming_losses, \n",
        "                      hamming_scores, hamming_score_2, acc, \n",
        "                      #f1_baru, presisi_baru, recall_baru\n",
        "                      ))\n",
        "        \n",
        "    StoreData.to_csv(\"result_{}_{}.csv\".format(name, str(iter)))\n",
        "    print(\"Data Telah Diexport\")\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsnwQ2sHhZBT",
        "colab_type": "text"
      },
      "source": [
        "##Special"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UnFQXt9hqLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen=200\n",
        "def att_lstm(cat, embedding_matrix):\n",
        "\n",
        "    inp = Input(shape=(200,))\n",
        "    x = Embedding(num_words, 200, weights=[embedding_matrix], trainable=False)(inp)\n",
        "    x = SpatialDropout1D(0.2)(x)\n",
        "    x = Bidirectional(LSTM(256, return_sequences=True))(x)\n",
        "    x = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(64, activation=\"relu\")(x)\n",
        "    x = Dense(cat, activation=\"sigmoid\")(x)\n",
        "    model = Model(inputs=inp, outputs=x)\n",
        "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLcLslPaLg2x",
        "colab_type": "text"
      },
      "source": [
        "###HS & ABUSIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOnhaXNrLmvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "76a4a139-7539-496b-9ea0-058d2ded64fa",
        "tags": []
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "#data = data.drop([\"Normal\",], axis=1)\n",
        "data = pd.read_csv('dataset_asli.csv', encoding='latin-1')\n",
        "\n",
        "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original', \n",
        "                                      1: 'replacement'})\n",
        "\n",
        "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
        "#data['Tweet'] = data['Tweet'].apply(clean_tweet)\n",
        "data['Tweet'].replace('', np.nan, inplace=True)\n",
        "data.dropna(subset=['Tweet'], inplace=True)\n",
        "\n",
        "data = data.drop([\"HS_Individual\", \"HS_Group\", \"HS_Religion\", \"HS_Race\", \"HS_Physical\", \"HS_Gender\", \"HS_Other\", \"HS_Weak\", \"HS_Moderate\", \"HS_Strong\",], axis=1)\n",
        "\n",
        "\n",
        "num_words = 10700\n",
        "maxlen = 200\n",
        "\n",
        "X = []\n",
        "sentences = list(data[\"Tweet\"])\n",
        "for sen in sentences:\n",
        "    X.append(sen)\n",
        "\n",
        "categories = list(data.columns.values)\n",
        "categories = categories[1:]\n",
        "print(categories)\n",
        "print(len(categories))\n",
        "\n",
        "data_label = data[categories]\n",
        "\n",
        "y = data_label.values\n",
        "len(y)\n",
        "\n",
        "\n",
        "model_ug_cbow = KeyedVectors.load('new_wv_cbow.word2vec')\n",
        "model_ug_sg = KeyedVectors.load('new_wv_skipgram.word2vec')\n",
        "\n",
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.vocab.keys():\n",
        "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
        "print('Ada %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "#X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = pad_sequences(X, padding='pre', maxlen=maxlen)\n",
        "#X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#import pickle\n",
        "\n",
        "# saving\n",
        "#with open('tokenizer_90.pickle', 'wb') as handle:\n",
        "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['HS', 'Abusive']\n2\nAda 7119 word vectors.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_hrhGa3NFYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "3bdb847d-0b7a-46cd-b63b-c5a0338f3c72",
        "tags": []
      },
      "source": [
        "from skmultilearn.model_selection.iterative_stratification import  iterative_train_test_split\n",
        "\n",
        "X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, 0.1)\n",
        "print(\"Data Train X = {} \".format(len(X_train)))\n",
        "print(\"Data Train Y = {} \".format(len(y_train)))\n",
        "print(\"Data Test X = {} \".format(len(X_test)))\n",
        "print(\"Data Test Y = {} \".format(len(y_test)))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data Train X = 11802 \nData Train Y = 11802 \nData Test X = 1312 \nData Test Y = 1312 \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPoNYNvchw70",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "outputId": "b85cf06d-5642-4232-d96f-bef982c81286",
        "tags": []
      },
      "source": [
        "#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "#mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
        "\n",
        "cvscores = []\n",
        "\n",
        "iterasi = 1\n",
        "Y = data_label[categories].values\n",
        "name = \"Main\"\n",
        "part = \"Main\"\n",
        "\n",
        "print(\"======================================================\")\n",
        "print(\"Iterasi = \"+str(iterasi)+\"\\n\")\n",
        "model = att_lstm(len(categories), embedding_matrix)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#callback\n",
        "#logdir = os.path.join(\"../log/\"+versi+\"/\"+part+\"/\"+\"att_bilstm\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_lstm_2 = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "filepath=\"Main_BiLSTM_Att.h5\"\n",
        "\n",
        "save_model = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "stop_train = EarlyStopping(monitor='val_loss', \n",
        "                          min_delta=0, patience=3, verbose=1, mode='min', \n",
        "                          baseline=None, restore_best_weights=True)\n",
        "\n",
        "callbacks_list = [\n",
        "                  #tensorboard_lstm_2, \n",
        "                  save_model,\n",
        "                  stop_train,\n",
        "                  ]\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=128, class_weight='balance',\n",
        "          verbose=1,validation_data=(X_test, y_test), \n",
        "          callbacks=callbacks_list\n",
        "          )\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "\n",
        "metric_sklearn(X_test, y_test, model, name, iterasi)\n",
        "\n",
        "#y_true = y_test\n",
        "#y_pred = (model.predict(X_test) >= 0.5).astype('int')\n",
        "#plot_multiclass_confusion_matrix(y_true, y_pred, iterasi, categories, name, save_plot=True)\n",
        "#f_score(X[test], Y[test], model, iterasi, name)\n",
        "\n",
        "\n",
        "\n",
        "  # create model\n",
        "  \n",
        "  #model.summary()\n",
        "  \n",
        "  # Compile model\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "======================================================\nIterasi = 1\n\nTrain on 11802 samples, validate on 1312 samples\nEpoch 1/100\n11802/11802 [==============================] - 89s 8ms/step - loss: 0.4172 - accuracy: 0.8070 - val_loss: 0.3512 - val_accuracy: 0.8464\n\nEpoch 00001: val_accuracy improved from -inf to 0.84642, saving model to Main_BiLSTM_Att.h5\nEpoch 2/100\n11802/11802 [==============================] - 86s 7ms/step - loss: 0.3290 - accuracy: 0.8536 - val_loss: 0.3101 - val_accuracy: 0.8655\n\nEpoch 00002: val_accuracy improved from 0.84642 to 0.86547, saving model to Main_BiLSTM_Att.h5\nEpoch 3/100\n11802/11802 [==============================] - 85s 7ms/step - loss: 0.2844 - accuracy: 0.8766 - val_loss: 0.2872 - val_accuracy: 0.8708\n\nEpoch 00003: val_accuracy improved from 0.86547 to 0.87081, saving model to Main_BiLSTM_Att.h5\nEpoch 4/100\n11802/11802 [==============================] - 85s 7ms/step - loss: 0.2546 - accuracy: 0.8885 - val_loss: 0.2971 - val_accuracy: 0.8746\n\nEpoch 00004: val_accuracy improved from 0.87081 to 0.87462, saving model to Main_BiLSTM_Att.h5\nEpoch 5/100\n11802/11802 [==============================] - 86s 7ms/step - loss: 0.2219 - accuracy: 0.9067 - val_loss: 0.3273 - val_accuracy: 0.8632\n\nEpoch 00005: val_accuracy did not improve from 0.87462\nEpoch 6/100\n11802/11802 [==============================] - 85s 7ms/step - loss: 0.1967 - accuracy: 0.9177 - val_loss: 0.2985 - val_accuracy: 0.8716\n\nEpoch 00006: val_accuracy did not improve from 0.87462\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n1312/1312 [==============================] - 13s 10ms/step\naccuracy: 87.08%\nFor threshold:  0.1\nMicro-average quality numbers\nPrecision: 0.6564, Recall: 0.9679, F1-measure: 0.7823, Jaccard: 0.6424, Hamming Loss: 0.2172, Hamming Score: 0.7161, Hamming Score 2: 0.7828, Accuracy: 0.6502,\nFor threshold:  0.2\nMicro-average quality numbers\nPrecision: 0.7200, Recall: 0.9405, F1-measure: 0.8156, Jaccard: 0.6886, Hamming Loss: 0.1715, Hamming Score: 0.7679, Hamming Score 2: 0.8285, Accuracy: 0.7066,\nFor threshold:  0.3\nMicro-average quality numbers\nPrecision: 0.7630, Recall: 0.9130, F1-measure: 0.8313, Jaccard: 0.7113, Hamming Loss: 0.1494, Hamming Score: 0.7938, Hamming Score 2: 0.8506, Accuracy: 0.7340,\nFor threshold:  0.4\nMicro-average quality numbers\nPrecision: 0.8043, Recall: 0.8819, F1-measure: 0.8413, Jaccard: 0.7261, Hamming Loss: 0.1341, Hamming Score: 0.8133, Hamming Score 2: 0.8659, Accuracy: 0.7607,\nFor threshold:  0.5\nMicro-average quality numbers\nPrecision: 0.8363, Recall: 0.8450, F1-measure: 0.8406, Jaccard: 0.7251, Hamming Loss: 0.1292, Hamming Score: 0.8220, Hamming Score 2: 0.8708, Accuracy: 0.7729,\nFor threshold:  0.6\nMicro-average quality numbers\nPrecision: 0.8764, Recall: 0.7977, F1-measure: 0.8352, Jaccard: 0.7171, Hamming Loss: 0.1269, Hamming Score: 0.8239, Hamming Score 2: 0.8731, Accuracy: 0.7797,\nFor threshold:  0.7\nMicro-average quality numbers\nPrecision: 0.9172, Recall: 0.7439, F1-measure: 0.8215, Jaccard: 0.6971, Hamming Loss: 0.1303, Hamming Score: 0.8148, Hamming Score 2: 0.8697, Accuracy: 0.7759,\nFor threshold:  0.8\nMicro-average quality numbers\nPrecision: 0.9471, Recall: 0.6427, F1-measure: 0.7658, Jaccard: 0.6204, Hamming Loss: 0.1585, Hamming Score: 0.7767, Hamming Score 2: 0.8415, Accuracy: 0.7340,\nFor threshold:  0.9\nMicro-average quality numbers\nPrecision: 0.9695, Recall: 0.5113, F1-measure: 0.6696, Jaccard: 0.5033, Hamming Loss: 0.2035, Hamming Score: 0.7149, Hamming Score 2: 0.7965, Accuracy: 0.6753,\nData Telah Diexport\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69VbIq13aWQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO3KTXtvOxAg",
        "colab_type": "text"
      },
      "source": [
        "###Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZgJuERkRsaH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "90c7370c-c493-450a-e0cb-b55057c6c5e2",
        "tags": []
      },
      "source": [
        "#data = data.drop([\"Normal\",], axis=1)\n",
        "data = pd.read_csv('dataset_asli.csv', encoding='latin-1')\n",
        "\n",
        "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original', \n",
        "                                      1: 'replacement'})\n",
        "\n",
        "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
        "#data['Tweet'] = data['Tweet'].apply(clean_tweet)\n",
        "data['Tweet'].replace('', np.nan, inplace=True)\n",
        "data.dropna(subset=['Tweet'], inplace=True)\n",
        "\n",
        "data = data.drop([\"HS\",\"Abusive\",\"HS_Religion\", \"HS_Race\", \"HS_Physical\", \"HS_Gender\", \"HS_Other\", \"HS_Weak\", \"HS_Moderate\", \"HS_Strong\",], axis=1)\n",
        "\n",
        "\n",
        "num_words = 10700\n",
        "maxlen = 200\n",
        "\n",
        "X = []\n",
        "sentences = list(data[\"Tweet\"])\n",
        "for sen in sentences:\n",
        "    X.append(sen)\n",
        "\n",
        "categories = list(data.columns.values)\n",
        "categories = categories[1:]\n",
        "print(categories)\n",
        "print(len(categories))\n",
        "\n",
        "data_label = data[categories]\n",
        "\n",
        "y = data_label.values\n",
        "len(y)\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "model_ug_cbow = KeyedVectors.load('new_wv_cbow.word2vec')\n",
        "model_ug_sg = KeyedVectors.load('new_wv_skipgram.word2vec')\n",
        "\n",
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.vocab.keys():\n",
        "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
        "print('Ada %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "#X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = pad_sequences(X, padding='pre', maxlen=maxlen)\n",
        "#X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#import pickle\n",
        "\n",
        "# saving\n",
        "#with open('tokenizer_90.pickle', 'wb') as handle:\n",
        "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#=================================================================================================\n",
        "from skmultilearn.model_selection.iterative_stratification import  iterative_train_test_split\n",
        "\n",
        "X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, 0.1)\n",
        "print(\"Data Train X = {} \".format(len(X_train)))\n",
        "print(\"Data Train Y = {} \".format(len(y_train)))\n",
        "print(\"Data Test X = {} \".format(len(X_test)))\n",
        "print(\"Data Test Y = {} \".format(len(y_test)))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['HS_Individual', 'HS_Group']\n2\nAda 7119 word vectors.\nData Train X = 11802 \nData Train Y = 11802 \nData Test X = 1312 \nData Test Y = 1312 \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lThepSKjag8R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71843647-4327-4418-e041-c3280b52a224",
        "tags": []
      },
      "source": [
        "\n",
        "cvscores = []\n",
        "\n",
        "iterasi = 1\n",
        "Y = data_label[categories].values\n",
        "name = \"BiLSTM_Att_Target\"\n",
        "part = \"Target\"\n",
        "\n",
        "print(\"======================================================\")\n",
        "print(\"Iterasi = \"+str(iterasi)+\"\\n\")\n",
        "model = att_lstm(len(categories), embedding_matrix)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#callback\n",
        "#logdir = os.path.join(\"/content/drive/My Drive/1.Skripsi/Projek/Baru/log/\"+versi+\"/\"+part+\"/\"+\"att_bilstm\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_lstm_2 = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "filepath=\"Target_BiLSTM_Att.h5\"\n",
        "\n",
        "save_model = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "stop_train = EarlyStopping(monitor='val_loss', \n",
        "                          min_delta=0, patience=3, verbose=1, mode='min', \n",
        "                          baseline=None, restore_best_weights=True)\n",
        "\n",
        "callbacks_list = [\n",
        "                  #tensorboard_lstm_2, \n",
        "                  save_model,\n",
        "                  stop_train,\n",
        "                  ]\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=128, class_weight='balance',\n",
        "          verbose=1,validation_data=(X_test, y_test), \n",
        "          callbacks=callbacks_list\n",
        "          )\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "\n",
        "#metric_sklearn(X_test, y_test, model, name, iterasi)\n",
        "\n",
        "#y_true = y_test\n",
        "#y_pred = (model.predict(X_test) >= 0.5).astype('int')\n",
        "#plot_multiclass_confusion_matrix(y_true, y_pred, iterasi, categories, name, save_plot=True)\n",
        "#f_score(X[test], Y[test], model, iterasi, name)\n",
        "\n",
        "\n",
        "\n",
        "  # create model\n",
        "  \n",
        "  #model.summary()\n",
        "  \n",
        "  # Compile model\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "======================================================\nIterasi = 1\n\nTrain on 11802 samples, validate on 1312 samples\nEpoch 1/100\n11802/11802 [==============================] - 97s 8ms/step - loss: 0.3972 - accuracy: 0.8172 - val_loss: 0.3524 - val_accuracy: 0.8457\n\nEpoch 00001: val_accuracy improved from -inf to 0.84566, saving model to Target_BiLSTM_Att.h5\nEpoch 2/100\n11802/11802 [==============================] - 94s 8ms/step - loss: 0.3416 - accuracy: 0.8456 - val_loss: 0.3507 - val_accuracy: 0.8472\n\nEpoch 00002: val_accuracy improved from 0.84566 to 0.84718, saving model to Target_BiLSTM_Att.h5\nEpoch 3/100\n11802/11802 [==============================] - 93s 8ms/step - loss: 0.3136 - accuracy: 0.8587 - val_loss: 0.3345 - val_accuracy: 0.8533\n\nEpoch 00003: val_accuracy improved from 0.84718 to 0.85328, saving model to Target_BiLSTM_Att.h5\nEpoch 4/100\n11802/11802 [==============================] - 93s 8ms/step - loss: 0.2856 - accuracy: 0.8732 - val_loss: 0.3400 - val_accuracy: 0.8586\n\nEpoch 00004: val_accuracy improved from 0.85328 to 0.85861, saving model to Target_BiLSTM_Att.h5\nEpoch 5/100\n11802/11802 [==============================] - 93s 8ms/step - loss: 0.2580 - accuracy: 0.8893 - val_loss: 0.3490 - val_accuracy: 0.8647\n\nEpoch 00005: val_accuracy improved from 0.85861 to 0.86471, saving model to Target_BiLSTM_Att.h5\nEpoch 6/100\n11802/11802 [==============================] - 93s 8ms/step - loss: 0.2293 - accuracy: 0.8988 - val_loss: 0.4110 - val_accuracy: 0.8529\n\nEpoch 00006: val_accuracy did not improve from 0.86471\nRestoring model weights from the end of the best epoch\nEpoch 00006: early stopping\n1312/1312 [==============================] - 15s 11ms/step\naccuracy: 85.33%\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4lSCe7RaqN6",
        "colab_type": "text"
      },
      "source": [
        "###Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSziSUwaasoh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "519604be-3621-4936-b3da-965bbbfd10d8",
        "tags": []
      },
      "source": [
        "#data = data.drop([\"Normal\",], axis=1)\n",
        "data = pd.read_csv('dataset_asli.csv', encoding='latin-1')\n",
        "\n",
        "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original', \n",
        "                                      1: 'replacement'})\n",
        "\n",
        "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
        "#data['Tweet'] = data['Tweet'].apply(clean_tweet)\n",
        "data['Tweet'].replace('', np.nan, inplace=True)\n",
        "data.dropna(subset=['Tweet'], inplace=True)\n",
        "\n",
        "\n",
        "#data = data.drop([\"HS_Individual\", \"HS_Group\", \"HS_Religion\", \"HS_Race\", \"HS_Physical\", \"HS_Gender\", \"HS_Other\", \"HS_Weak\", \"HS_Moderate\", \"HS_Strong\",], axis=1)\n",
        "\n",
        "data = data.drop([\"HS\",\"Abusive\",\"HS_Individual\", \"HS_Group\", \"HS_Weak\", \"HS_Moderate\", \"HS_Strong\",], axis=1)\n",
        "\n",
        "\n",
        "num_words = 10700\n",
        "maxlen = 200\n",
        "\n",
        "X = []\n",
        "sentences = list(data[\"Tweet\"])\n",
        "for sen in sentences:\n",
        "    X.append(sen)\n",
        "\n",
        "categories = list(data.columns.values)\n",
        "categories = categories[1:]\n",
        "print(categories)\n",
        "print(len(categories))\n",
        "\n",
        "data_label = data[categories]\n",
        "\n",
        "y = data_label.values\n",
        "len(y)\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "model_ug_cbow = KeyedVectors.load('new_wv_cbow.word2vec')\n",
        "model_ug_sg = KeyedVectors.load('new_wv_skipgram.word2vec')\n",
        "\n",
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.vocab.keys():\n",
        "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
        "print('Ada %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "#X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = pad_sequences(X, padding='pre', maxlen=maxlen)\n",
        "#X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#import pickle\n",
        "\n",
        "# saving\n",
        "#with open('tokenizer_90.pickle', 'wb') as handle:\n",
        "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#=================================================================================================\n",
        "from skmultilearn.model_selection.iterative_stratification import  iterative_train_test_split\n",
        "\n",
        "X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, 0.1)\n",
        "print(\"Data Train X = {} \".format(len(X_train)))\n",
        "print(\"Data Train Y = {} \".format(len(y_train)))\n",
        "print(\"Data Test X = {} \".format(len(X_test)))\n",
        "print(\"Data Test Y = {} \".format(len(y_test)))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other']\n5\nAda 7119 word vectors.\nData Train X = 11802 \nData Train Y = 11802 \nData Test X = 1312 \nData Test Y = 1312 \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79MXMnnqSLbw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9090f51-53ab-4280-ca4c-ecb6c5e24d76",
        "tags": []
      },
      "source": [
        "\n",
        "cvscores = []\n",
        "\n",
        "iterasi = 1\n",
        "Y = data_label[categories].values\n",
        "name = \"BiLSTM_Att_type\"\n",
        "part = \"Type\"\n",
        "\n",
        "print(\"======================================================\")\n",
        "print(\"Iterasi = \"+str(iterasi)+\"\\n\")\n",
        "model = att_lstm(len(categories), embedding_matrix)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#callback\n",
        "#logdir = os.path.join(\"/content/drive/My Drive/1.Skripsi/Projek/Baru/log/\"+versi+\"/\"+part+\"/\"+\"att_bilstm\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_lstm_2 = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "filepath=\"Type_BiLSTM_Att.h5\"\n",
        "\n",
        "save_model = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "stop_train = EarlyStopping(monitor='val_loss', \n",
        "                          min_delta=0, patience=3, verbose=1, mode='min', \n",
        "                          baseline=None, restore_best_weights=True)\n",
        "\n",
        "callbacks_list = [\n",
        "                  #tensorboard_lstm_2, \n",
        "                  save_model,\n",
        "                  stop_train,\n",
        "                  ]\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=128, class_weight='balance',\n",
        "          verbose=1,validation_data=(X_test, y_test), \n",
        "          callbacks=callbacks_list\n",
        "          )\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "\n",
        "#metric_sklearn(X_test, y_test, model, name, iterasi)\n",
        "\n",
        "#y_true = y_test\n",
        "#y_pred = (model.predict(X_test) >= 0.5).astype('int')\n",
        "#plot_multiclass_confusion_matrix(y_true, y_pred, iterasi, categories, name, save_plot=True)\n",
        "#f_score(X[test], Y[test], model, iterasi, name)\n",
        "\n",
        "\n",
        "\n",
        "  # create model\n",
        "  \n",
        "  #model.summary()\n",
        "  \n",
        "  # Compile model\n",
        "\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "======================================================\nIterasi = 1\n\nTrain on 11802 samples, validate on 1312 samples\nEpoch 1/100\n11802/11802 [==============================] - 92s 8ms/step - loss: 0.1860 - accuracy: 0.9276 - val_loss: 0.1467 - val_accuracy: 0.9457\n\nEpoch 00001: val_accuracy improved from -inf to 0.94573, saving model to Type_BiLSTM_Att.h5\nEpoch 2/100\n11802/11802 [==============================] - 88s 7ms/step - loss: 0.1372 - accuracy: 0.9459 - val_loss: 0.1435 - val_accuracy: 0.9462\n\nEpoch 00002: val_accuracy improved from 0.94573 to 0.94619, saving model to Type_BiLSTM_Att.h5\nEpoch 3/100\n11802/11802 [==============================] - 89s 8ms/step - loss: 0.1168 - accuracy: 0.9539 - val_loss: 0.1400 - val_accuracy: 0.9463\n\nEpoch 00003: val_accuracy improved from 0.94619 to 0.94634, saving model to Type_BiLSTM_Att.h5\nEpoch 4/100\n11802/11802 [==============================] - 89s 8ms/step - loss: 0.1038 - accuracy: 0.9591 - val_loss: 0.1441 - val_accuracy: 0.9459\n\nEpoch 00004: val_accuracy did not improve from 0.94634\nEpoch 5/100\n11802/11802 [==============================] - 89s 8ms/step - loss: 0.0923 - accuracy: 0.9625 - val_loss: 0.1397 - val_accuracy: 0.9483\n\nEpoch 00005: val_accuracy improved from 0.94634 to 0.94832, saving model to Type_BiLSTM_Att.h5\nEpoch 6/100\n11802/11802 [==============================] - 89s 8ms/step - loss: 0.0790 - accuracy: 0.9678 - val_loss: 0.1495 - val_accuracy: 0.9492\n\nEpoch 00006: val_accuracy improved from 0.94832 to 0.94924, saving model to Type_BiLSTM_Att.h5\nEpoch 7/100\n11802/11802 [==============================] - 88s 7ms/step - loss: 0.0685 - accuracy: 0.9722 - val_loss: 0.1543 - val_accuracy: 0.9532\n\nEpoch 00007: val_accuracy improved from 0.94924 to 0.95320, saving model to Type_BiLSTM_Att.h5\nEpoch 8/100\n11802/11802 [==============================] - 88s 7ms/step - loss: 0.0572 - accuracy: 0.9770 - val_loss: 0.1639 - val_accuracy: 0.9509\n\nEpoch 00008: val_accuracy did not improve from 0.95320\nRestoring model weights from the end of the best epoch\nEpoch 00008: early stopping\n1312/1312 [==============================] - 13s 10ms/step\naccuracy: 94.83%\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xcn1l5pbPkA",
        "colab_type": "text"
      },
      "source": [
        "###Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQCuYxgwbR5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "5bf4363e-671a-42a8-8b5d-b9d95e235b39",
        "tags": []
      },
      "source": [
        "#data = data.drop([\"Normal\",], axis=1)\n",
        "data = pd.read_csv('dataset_asli.csv', encoding='latin-1')\n",
        "\n",
        "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
        "alay_dict = alay_dict.rename(columns={0: 'original', \n",
        "                                      1: 'replacement'})\n",
        "\n",
        "data['Tweet'] = data['Tweet'].apply(preprocess)\n",
        "#data['Tweet'] = data['Tweet'].apply(clean_tweet)\n",
        "data['Tweet'].replace('', np.nan, inplace=True)\n",
        "data.dropna(subset=['Tweet'], inplace=True)\n",
        "\n",
        "\n",
        "#data = data.drop([\"HS_Individual\", \"HS_Group\", \"HS_Religion\", \"HS_Race\", \"HS_Physical\", \"HS_Gender\", \"HS_Other\", \"HS_Weak\", \"HS_Moderate\", \"HS_Strong\",], axis=1)\n",
        "\n",
        "data = data.drop([\"HS\",\"Abusive\",\"HS_Individual\", \"HS_Group\", \"HS_Religion\", \"HS_Race\", \"HS_Physical\", \"HS_Gender\", \"HS_Other\",], axis=1)\n",
        "\n",
        "\n",
        "num_words = 10700\n",
        "maxlen = 200\n",
        "\n",
        "X = []\n",
        "sentences = list(data[\"Tweet\"])\n",
        "for sen in sentences:\n",
        "    X.append(sen)\n",
        "\n",
        "categories = list(data.columns.values)\n",
        "categories = categories[1:]\n",
        "print(categories)\n",
        "print(len(categories))\n",
        "\n",
        "data_label = data[categories]\n",
        "\n",
        "y = data_label.values\n",
        "len(y)\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "model_ug_cbow = KeyedVectors.load('new_wv_cbow.word2vec')\n",
        "model_ug_sg = KeyedVectors.load('new_wv_skipgram.word2vec')\n",
        "\n",
        "embeddings_index = {}\n",
        "for w in model_ug_cbow.wv.vocab.keys():\n",
        "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
        "print('Ada %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token=\"OOV\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "#X_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "X = pad_sequences(X, padding='pre', maxlen=maxlen)\n",
        "#X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
        "\n",
        "#import pickle\n",
        "\n",
        "# saving\n",
        "#with open('tokenizer_90.pickle', 'wb') as handle:\n",
        "#    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, 200))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "#=================================================================================================\n",
        "from skmultilearn.model_selection.iterative_stratification import  iterative_train_test_split\n",
        "\n",
        "X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, 0.1)\n",
        "print(\"Data Train X = {} \".format(len(X_train)))\n",
        "print(\"Data Train Y = {} \".format(len(y_train)))\n",
        "print(\"Data Test X = {} \".format(len(X_test)))\n",
        "print(\"Data Test Y = {} \".format(len(y_test)))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['HS_Weak', 'HS_Moderate', 'HS_Strong']\n3\nAda 7119 word vectors.\nData Train X = 11802 \nData Train Y = 11802 \nData Test X = 1312 \nData Test Y = 1312 \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxvQKqpsbe5L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "680302f6-5711-4e4b-b945-9857057d183f",
        "tags": []
      },
      "source": [
        "\n",
        "\n",
        "cvscores = []\n",
        "\n",
        "iterasi = 1\n",
        "Y = data_label[categories].values\n",
        "name = \"BiLSTM_Att_Level\"\n",
        "part = \"Level\"\n",
        "\n",
        "print(\"======================================================\")\n",
        "print(\"Iterasi = \"+str(iterasi)+\"\\n\")\n",
        "model = att_lstm(len(categories), embedding_matrix)\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "#callback\n",
        "#logdir = os.path.join(\"/content/drive/My Drive/1.Skripsi/Projek/Baru/log/\"+versi+\"/\"+part+\"/\"+\"att_bilstm\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "#tensorboard_lstm_2 = TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "filepath=\"Level_BiLSTM_Att.h5\"\n",
        "\n",
        "save_model = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "stop_train = EarlyStopping(monitor='val_loss', \n",
        "                          min_delta=0, patience=3, verbose=1, mode='min', \n",
        "                          baseline=None, restore_best_weights=True)\n",
        "\n",
        "callbacks_list = [\n",
        "                  #tensorboard_lstm_2, \n",
        "                  save_model,\n",
        "                  stop_train,\n",
        "                  ]\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train, epochs=100, batch_size=128, class_weight='balance',\n",
        "          verbose=1,validation_data=(X_test, y_test), \n",
        "          callbacks=callbacks_list\n",
        "          )\n",
        "# evaluate the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "\n",
        "\n",
        "#metric_sklearn(X_test, y_test, model, name, iterasi)\n",
        "\n",
        "#y_true = y_test\n",
        "#y_pred = (model.predict(X_test) >= 0.5).astype('int')\n",
        "#plot_multiclass_confusion_matrix(y_true, y_pred, iterasi, categories, name, save_plot=True)\n",
        "#f_score(X[test], Y[test], model, iterasi, name)\n",
        "\n",
        "\n",
        "\n",
        "  # create model\n",
        "  \n",
        "  #model.summary()\n",
        "  \n",
        "  # Compile model\n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "======================================================\nIterasi = 1\n\nTrain on 11802 samples, validate on 1312 samples\nEpoch 1/100\n11802/11802 [==============================] - 87s 7ms/step - loss: 0.3012 - accuracy: 0.8699 - val_loss: 0.2553 - val_accuracy: 0.8877\n\nEpoch 00001: val_accuracy improved from -inf to 0.88770, saving model to Level_BiLSTM_Att.h5\nEpoch 2/100\n11802/11802 [==============================] - 86s 7ms/step - loss: 0.2520 - accuracy: 0.8894 - val_loss: 0.2558 - val_accuracy: 0.8874\n\nEpoch 00002: val_accuracy did not improve from 0.88770\nEpoch 3/100\n11802/11802 [==============================] - 86s 7ms/step - loss: 0.2284 - accuracy: 0.9000 - val_loss: 0.2508 - val_accuracy: 0.8918\n\nEpoch 00003: val_accuracy improved from 0.88770 to 0.89177, saving model to Level_BiLSTM_Att.h5\nEpoch 4/100\n11802/11802 [==============================] - 85s 7ms/step - loss: 0.2089 - accuracy: 0.9113 - val_loss: 0.2458 - val_accuracy: 0.8974\n\nEpoch 00004: val_accuracy improved from 0.89177 to 0.89736, saving model to Level_BiLSTM_Att.h5\nEpoch 5/100\n11802/11802 [==============================] - 86s 7ms/step - loss: 0.1849 - accuracy: 0.9213 - val_loss: 0.2645 - val_accuracy: 0.8930\n\nEpoch 00005: val_accuracy did not improve from 0.89736\nEpoch 6/100\n11802/11802 [==============================] - 86s 7ms/step - loss: 0.1647 - accuracy: 0.9302 - val_loss: 0.2709 - val_accuracy: 0.8971\n\nEpoch 00006: val_accuracy did not improve from 0.89736\nEpoch 7/100\n11802/11802 [==============================] - 86s 7ms/step - loss: 0.1414 - accuracy: 0.9403 - val_loss: 0.2920 - val_accuracy: 0.8953\n\nEpoch 00007: val_accuracy did not improve from 0.89736\nRestoring model weights from the end of the best epoch\nEpoch 00007: early stopping\n1312/1312 [==============================] - 13s 10ms/step\naccuracy: 89.74%\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mifw73X6lkG9",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmRl2oaXsV9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files \n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-_ckCd2p4n7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "53a0d119-4edd-4ded-c572-c8aaccb5217e"
      },
      "source": [
        "from keras.models import load_model\n",
        "import pickle\n",
        "maxlen = 200\n",
        "tokenizer = Tokenizer(5000)\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "#categories = list(data.columns.values)\n",
        "#categories = categories[1:]\n",
        "#print(categories)\n",
        "#%cd \"/content/drive/My Drive/1.Skripsi/Projek/Baru/Model/Rev4/\"\n",
        "\n",
        "lstm = load_model('10_LSTM_Normal.h5')\n",
        "lstm_w2v = load_model('10_LSTM_W2V.h5')\n",
        "\n",
        "cnn = load_model('10_CNN_Normal.h5')\n",
        "cnn_w2v = load_model('10_CNN_W2V.h5')\n",
        "\n",
        "#%cd \"/content/drive/\"\n",
        "\n",
        "def prediction_to_label(prediction):\n",
        "    tag_prob = [(categories[i], prob) for i, prob in enumerate(prediction.tolist())]\n",
        "    return dict(sorted(tag_prob, key=lambda kv: kv[1], reverse=False))\n",
        "\n",
        "def get_features(text_series):\n",
        "    \"\"\"\n",
        "    transforms text data to feature_vectors that can be used in the ml model.\n",
        "    tokenizer must be available.\n",
        "    \"\"\"\n",
        "    sequences = tokenizer.texts_to_sequences(text_series)\n",
        "    return pad_sequences(sequences, maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "on_uqmr3rJwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd \"/content/drive/My Drive/1.Skripsi/Projek/Baru/Model/Rev4/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRU5pKqjp0Bh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = pd.read_csv('dataset_10.csv', encoding='latin-1')\n",
        "test['Tweet'] = test['Tweet'].apply(preprocess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8P17BFGprQO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cc63768b-d4ab-4bbc-8572-13936a554734"
      },
      "source": [
        "X_test = []\n",
        "sentences = list(test[\"Tweet\"])\n",
        "for sen in sentences:\n",
        "    X_test.append(sen)\n",
        "\n",
        "categories = list(test.columns.values)\n",
        "categories = categories[1:]\n",
        "print(categories)\n",
        "print(len(categories))\n",
        "\n",
        "data_label = test[categories]\n",
        "\n",
        "y_test = data_label.values\n",
        "len(y_test)\n",
        "\n",
        "X_test = get_features(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn6HEQ1tqGAy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b1bf1b34-818d-4b1e-a69f-a4acb6084027"
      },
      "source": [
        "text = \"Cina Asu\"\n",
        "\n",
        "q = preprocess(text)\n",
        "\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)\n",
        "\n",
        "f =  get_features([q])\n",
        "print(f)\n",
        "\n",
        "p1 = prediction_to_label(lstm.predict(f)[0])\n",
        "p2 = prediction_to_label(lstm_w2v.predict(f)[0])\n",
        "p3 = prediction_to_label(cnn.predict(f)[0])\n",
        "p4 = prediction_to_label(cnn_w2v.predict(f)[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  0  0  0  0  0 19 60]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb9A1hGJqHVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "76918dcf-34c6-450a-80ac-11899de32196"
      },
      "source": [
        "df = pd.DataFrame()\n",
        "df['label'] = p1.keys()\n",
        "df['lstm'] = p1.values()\n",
        "df['lstm_w2v'] = df.label.apply(lambda label : p2.get(label))\n",
        "df['weighted_lstm'] = (2 * df['lstm_w2v'] + df['lstm']) / 3\n",
        "df['cnn'] = df.label.apply(lambda label : p3.get(label))\n",
        "df['cnn_w2v'] = df.label.apply(lambda label : p4.get(label))\n",
        "df['weighted_cnn'] = (2 * df['cnn_w2v'] + df['cnn']) / 3\n",
        "df['both'] = (2 * df['weighted_cnn'] + df['weighted_lstm']) / 3\n",
        "df.sort_values(by='lstm_w2v', ascending=False)[:4]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>lstm</th>\n",
              "      <th>lstm_w2v</th>\n",
              "      <th>weighted_lstm</th>\n",
              "      <th>cnn</th>\n",
              "      <th>cnn_w2v</th>\n",
              "      <th>weighted_cnn</th>\n",
              "      <th>both</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>HS_Moderate</td>\n",
              "      <td>0.035833</td>\n",
              "      <td>0.529290</td>\n",
              "      <td>0.364805</td>\n",
              "      <td>0.121054</td>\n",
              "      <td>0.017171</td>\n",
              "      <td>0.051798</td>\n",
              "      <td>0.156134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>HS_Other</td>\n",
              "      <td>0.245857</td>\n",
              "      <td>0.527902</td>\n",
              "      <td>0.433887</td>\n",
              "      <td>0.375397</td>\n",
              "      <td>0.135330</td>\n",
              "      <td>0.215352</td>\n",
              "      <td>0.288197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>HS</td>\n",
              "      <td>0.165466</td>\n",
              "      <td>0.526290</td>\n",
              "      <td>0.406015</td>\n",
              "      <td>0.559074</td>\n",
              "      <td>0.227842</td>\n",
              "      <td>0.338253</td>\n",
              "      <td>0.360840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>HS_Group</td>\n",
              "      <td>0.031684</td>\n",
              "      <td>0.514805</td>\n",
              "      <td>0.353765</td>\n",
              "      <td>0.127919</td>\n",
              "      <td>0.013949</td>\n",
              "      <td>0.051939</td>\n",
              "      <td>0.152547</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          label      lstm  lstm_w2v  ...   cnn_w2v  weighted_cnn      both\n",
              "6   HS_Moderate  0.035833  0.529290  ...  0.017171      0.051798  0.156134\n",
              "11     HS_Other  0.245857  0.527902  ...  0.135330      0.215352  0.288197\n",
              "7            HS  0.165466  0.526290  ...  0.227842      0.338253  0.360840\n",
              "5      HS_Group  0.031684  0.514805  ...  0.013949      0.051939  0.152547\n",
              "\n",
              "[4 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ7iSxOfaALa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optional\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=9000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YREywFy16y59",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "59ba491a-4157-4d47-a3c8-a630d51cef9b"
      },
      "source": [
        "cnn_w2v.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1317/1317 [==============================] - 6s 5ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8900064979888493, 0.826309859752655]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQK4xypuDUQY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b21f117c-8e5f-43d0-c47b-b360df607fc0"
      },
      "source": [
        "cnn.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1317/1317 [==============================] - 0s 164us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4595354625286256, 0.7342445254325867]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rnmBNOzDQiU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "515875ef-a77a-45a9-8cb6-2602b7cb3328"
      },
      "source": [
        "lstm_w2v.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1317/1317 [==============================] - 2s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4344664118737096, 0.32890406250953674]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBihN4hlaI30",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "f8b98241-cc63-4bd8-de65-16e9ef33146f"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "predictions=lstm.predict(X_test)\n",
        "thresholds=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "for val in thresholds:\n",
        "    pred=predictions.copy()\n",
        "  \n",
        "    pred[pred>=val]=1\n",
        "    pred[pred<val]=0\n",
        "  \n",
        "    precision = precision_score(y_test, pred, average='macro')\n",
        "    recall = recall_score(y_test, pred, average='macro')\n",
        "    f1 = f1_score(y_test, pred, average='macro')\n",
        "    print(\"Treshold = \"+str(val))   \n",
        "    print(\"Micro-average quality numbers\")\n",
        "    print(\"Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f} \\n\".format(precision, recall, f1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Treshold = 0.1\n",
            "Micro-average quality numbers\n",
            "Precision: 0.1544, Recall: 0.4258, F1-measure: 0.2115 \n",
            "\n",
            "Treshold = 0.2\n",
            "Micro-average quality numbers\n",
            "Precision: 0.1327, Recall: 0.4167, F1-measure: 0.1997 \n",
            "\n",
            "Treshold = 0.3\n",
            "Micro-average quality numbers\n",
            "Precision: 0.1228, Recall: 0.2815, F1-measure: 0.1661 \n",
            "\n",
            "Treshold = 0.4\n",
            "Micro-average quality numbers\n",
            "Precision: 0.0924, Recall: 0.0377, F1-measure: 0.0488 \n",
            "\n",
            "Treshold = 0.5\n",
            "Micro-average quality numbers\n",
            "Precision: 0.0419, Recall: 0.0044, F1-measure: 0.0071 \n",
            "\n",
            "Treshold = 0.6\n",
            "Micro-average quality numbers\n",
            "Precision: 0.0417, Recall: 0.0008, F1-measure: 0.0016 \n",
            "\n",
            "Treshold = 0.7\n",
            "Micro-average quality numbers\n",
            "Precision: 0.0238, Recall: 0.0005, F1-measure: 0.0009 \n",
            "\n",
            "Treshold = 0.8\n",
            "Micro-average quality numbers\n",
            "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000 \n",
            "\n",
            "Treshold = 0.9\n",
            "Micro-average quality numbers\n",
            "Precision: 0.0000, Recall: 0.0000, F1-measure: 0.0000 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z91S1RwNSkR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skmultilearn.model_selection.iterative_stratification import iterative_train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = iterative_train_test_split(X, y, test_size=0.1,)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP4gH_uvY8dJ",
        "colab_type": "text"
      },
      "source": [
        "#Test Kfold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzWCOhgjY_oA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "182d5521-1ebe-45ff-bfe5-bfde5804157a"
      },
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "skfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=5959)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=6969)\n",
        "mskf = MultilabelStratifiedKFold(n_splits=10, random_state=0)\n",
        "cvscores = []\n",
        "iterasi = 1\n",
        "\n",
        "from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n",
        "                                     StratifiedKFold, GroupShuffleSplit,\n",
        "                                     GroupKFold, StratifiedShuffleSplit)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "np.random.seed(1338)\n",
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm\n",
        "n_splits = 4\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "print(len(X))\n",
        "Y = data_label[categories].values\n",
        "print(len(Y))\n",
        "\n",
        "def plot_cv_indices(cv, X, y , ax, n_splits, lw=10):\n",
        "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
        "\n",
        "    # Generate the training/testing visualizations for each CV split\n",
        "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, )):\n",
        "        # Fill in indices with the training/test groups\n",
        "        indices = np.array([np.nan] * len(X))\n",
        "        indices[tt] = 1\n",
        "        indices[tr] = 0\n",
        "\n",
        "        # Visualize the results\n",
        "        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n",
        "                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n",
        "                   vmin=-.2, vmax=1.2)\n",
        "\n",
        "    # Plot the data classes and groups at the end\n",
        "    #ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n",
        "    #           c=y, marker='_', lw=lw, cmap=cmap_data)\n",
        "\n",
        "    #ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n",
        "    #           c=group, marker='_', lw=lw, cmap=cmap_data)\n",
        "\n",
        "    # Formatting\n",
        "    #yticklabels = list(range(n_splits)) + ['class', 'group']\n",
        "    ax.set(#yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n",
        "           xlabel='Jumlah Data', ylabel=\"CV Iterasi\",\n",
        "           #ylim=[n_splits+2.2, -.2], xlim=[0, 100]\n",
        "           )\n",
        "    ax.set_title(\"Visualisasi Pembagian Data\", fontsize=15)\n",
        "    return ax\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-744fd0cee87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0miterstrat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml_stratifiers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultilabelStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mskfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5959\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6969\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmskf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultilabelStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'iterstrat'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjKqS468RJNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "outputId": "f3c8c909-0541-4388-a4e2-0ac2e76ba5da"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "cv = kfold(10)\n",
        "n_splits = 10\n",
        "plot_cv_indices(cv, X, Y, ax, n_splits)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4321f3906fdf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplot_cv_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'KFold' object is not callable"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4z9hVbaNxL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "15de277e-ac58-4ee0-9b34-fc6bc9d4db46"
      },
      "source": [
        "from sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n",
        "                                     StratifiedKFold, GroupShuffleSplit,\n",
        "                                     GroupKFold, StratifiedShuffleSplit)\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "np.random.seed(1338)\n",
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm\n",
        "n_splits = 4\n",
        "\n",
        "n_points = 100\n",
        "X = np.random.randn(100, 10)\n",
        "\n",
        "percentiles_classes = [.1, .3, .6]\n",
        "y = np.hstack([[ii] * int(100 * perc)\n",
        "               for ii, perc in enumerate(percentiles_classes)])\n",
        "\n",
        "# Evenly spaced groups repeated once\n",
        "groups = np.hstack([[ii] * 10 for ii in range(10)])\n",
        "\n",
        "\n",
        "def visualize_groups(classes, groups, name):\n",
        "    # Visualize dataset groups\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(range(len(groups)),  [.5] * len(groups), c=groups, marker='_',\n",
        "               lw=50, cmap=cmap_data)\n",
        "    ax.scatter(range(len(groups)),  [3.5] * len(groups), c=classes, marker='_',\n",
        "               lw=50, cmap=cmap_data)\n",
        "    ax.set(ylim=[-1, 5], yticks=[.5, 3.5],\n",
        "           yticklabels=['Data\\ngroup', 'Data\\nclass'], xlabel=\"Sample index\")\n",
        "\n",
        "\n",
        "#visualize_groups(y, groups, 'no groups')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3\n",
            " 3 3 3 4 4 4 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 6 6 6 6 6 6 6 6 6 6 7 7 7 7\n",
            " 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjTqpDpoUtFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "y_true = np.array([[0,1,0],\n",
        "                   [0,1,1],\n",
        "                   [1,0,1],\n",
        "                   [0,0,1],\n",
        "                   [1,1,1],])\n",
        "\n",
        "y_pred = np.array([[0,1,1],\n",
        "                   [0,1,1],\n",
        "                   [0,1,0],\n",
        "                   [0,0,0],\n",
        "                   [1,0,1],])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZWS0QJMUwfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    '''\n",
        "    Compute the Hamming score (a.k.a. label-based accuracy) for the multi-label case\n",
        "    http://stackoverflow.com/q/32239577/395857\n",
        "    '''\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        print('\\nset_true: {0}'.format(set_true))\n",
        "        print('set_pred: {0}'.format(set_pred))\n",
        "        tmp_a = None\n",
        "        \n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        print('tmp_a: {0}'.format(tmp_a))\n",
        "        acc_list.append(tmp_a)\n",
        "        print(acc_list)\n",
        "    return np.mean(acc_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk6FQeQ5U3T9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "6f0b0c62-bb78-4da8-d3e5-456810321a7f"
      },
      "source": [
        "hamming_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "set_true: {1}\n",
            "set_pred: {1, 2}\n",
            "tmp_a: 0.5\n",
            "[0.5]\n",
            "\n",
            "set_true: {1, 2}\n",
            "set_pred: {1, 2}\n",
            "tmp_a: 1.0\n",
            "[0.5, 1.0]\n",
            "\n",
            "set_true: {0, 2}\n",
            "set_pred: {1}\n",
            "tmp_a: 0.0\n",
            "[0.5, 1.0, 0.0]\n",
            "\n",
            "set_true: {2}\n",
            "set_pred: set()\n",
            "tmp_a: 0.0\n",
            "[0.5, 1.0, 0.0, 0.0]\n",
            "\n",
            "set_true: {0, 1, 2}\n",
            "set_pred: {0, 2}\n",
            "tmp_a: 0.6666666666666666\n",
            "[0.5, 1.0, 0.0, 0.0, 0.6666666666666666]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4333333333333333"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}